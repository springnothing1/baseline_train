{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import llama\n",
    "import clipvpr\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, process = clipvpr.load(clip_name=\"ViT-B/16\", device = device, llama_name=\"BIAS-7B\", llama_dir='./path/to/LLaMA/', llama_type=\"7B\", \n",
    "         llama_download_root='ckpts', max_seq_len=512, phase=\"finetune\", \n",
    "         prompt=['Is the scene in the picture urban or rural? How many lanes are there on the road in the photo? Is there a residential building in the picture? If so, which side of the road is it located on? Are there vegetation and trees in the photo? If so, which side of the road is it located on?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVPR(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (llama_model): LLaMA_adapter(\n",
       "    (clip): CLIP(\n",
       "      (visual): VisionTransformer(\n",
       "        (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer): Transformer(\n",
       "          (resblocks): Sequential(\n",
       "            (0): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (10): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (11): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (12): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (13): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (14): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (15): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (16): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (17): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (18): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (19): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (20): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (21): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (22): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (23): ResidualAttentionBlock(\n",
       "              (attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Sequential(\n",
       "                (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (gelu): QuickGELU()\n",
       "                (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (clip_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (clip_proj_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (visual_query): Embedding(10, 768)\n",
       "    (visual_blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
       "    (visual_proj_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (adapter_query): Embedding(310, 4096)\n",
       "    (llama): Transformer(\n",
       "      (tok_embeddings): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x TransformerBlock(\n",
       "          (attention): Attention(\n",
       "            (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "            (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "            (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          )\n",
       "          (attention_norm): RMSNorm()\n",
       "          (ffn_norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "      (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "prompts_list = 2 * ['Is there a sidewalk on the road in the picture?']\n",
    "\n",
    "prompts = [llama.format_prompt(prompt) for prompt in prompts_list]\n",
    "img1 = Image.fromarray(cv2.imread(\"./data/image1.jpg\"))\n",
    "img2 = Image.fromarray(cv2.imread(\"./data/image2.jpg\"))\n",
    "imgs = [img1, img2]\n",
    "img_load = torch.stack([process(img).to(device) for img in imgs], dim=0)\n",
    "print(img_load.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input \n['The scene in the picture is rural. There are two lanes on the road in the photo. There is a residential building in the picture. It is located on the right side of the road. There are vegetation and trees in the photo. It is located on the right side of the road.']\n\n### Instruction:\n['Is the scene in the picture urban or rural? How many lanes are there on the road in the photo? Is there a residential building in the picture? If so, which side of the road is it located on? Are there vegetation and trees in the photo? If so, which side of the road is it located on?']\n\n### Response:\n['The scene in the picture is rural. There are two lanes on the road in the photo. There is a residential building in the picture. It is located on the right side of the road. There are vegetation and trees in the photo. It is located on the right side of the road.']\n\n### Instruction:\n['Is the scene in the picture urban or rural? How many lanes are there on the road in the photo? Is there a residential building in the picture is too long for context length 77",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/baseline_train/clipvpr.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/baseline_train/clipvpr.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m model(img_load)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/baseline_train/clipvpr/model.py:382\u001b[0m, in \u001b[0;36mCLIPVPR.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    379\u001b[0m image_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_llama(image)\n\u001b[1;32m    380\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(image_text))\n\u001b[0;32m--> 382\u001b[0m image_text_tokens \u001b[39m=\u001b[39m tokenize(image_text)\n\u001b[1;32m    384\u001b[0m text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_text(image_text_tokens)\n\u001b[1;32m    386\u001b[0m fused_feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((image_features, text_features),dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/baseline_train/clipvpr/utils.py:125\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(texts, context_length, truncate)\u001b[0m\n\u001b[1;32m    123\u001b[0m             tokens[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m eot_token\n\u001b[1;32m    124\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtexts[i]\u001b[39m}\u001b[39;00m\u001b[39m is too long for context length \u001b[39m\u001b[39m{\u001b[39;00mcontext_length\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m     result[i, :\u001b[39mlen\u001b[39m(tokens)] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tokens)\n\u001b[1;32m    128\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input \n['The scene in the picture is rural. There are two lanes on the road in the photo. There is a residential building in the picture. It is located on the right side of the road. There are vegetation and trees in the photo. It is located on the right side of the road.']\n\n### Instruction:\n['Is the scene in the picture urban or rural? How many lanes are there on the road in the photo? Is there a residential building in the picture? If so, which side of the road is it located on? Are there vegetation and trees in the photo? If so, which side of the road is it located on?']\n\n### Response:\n['The scene in the picture is rural. There are two lanes on the road in the photo. There is a residential building in the picture. It is located on the right side of the road. There are vegetation and trees in the photo. It is located on the right side of the road.']\n\n### Instruction:\n['Is the scene in the picture urban or rural? How many lanes are there on the road in the photo? Is there a residential building in the picture is too long for context length 77"
     ]
    }
   ],
   "source": [
    "result = model(img_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
