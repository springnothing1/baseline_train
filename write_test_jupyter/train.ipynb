{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce188d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from GeMPooling import GeMPooling \n",
    "from d2l import torch as d2l\n",
    "from mapillary_sls.datasets.msls import MSLS\n",
    "from mapillary_sls.datasets.generic_dataset import ImagesFromList\n",
    "from mapillary_sls.utils.utils import configure_transform\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3bf16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_CITIES = \"zurich,sf\"\n",
    "\n",
    "root_dir = Path('/datasets/msls').absolute()\n",
    "\n",
    "# get transform\n",
    "meta = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
    "transform = configure_transform(image_dim = (480, 640), meta = meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f696d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> zurich\n",
      "=====> sf\n",
      "#Sideways [179/3021]; #Night; [0/3021]\n",
      "Forward and Day weighted with 1.0000\n",
      "Sideways and Day weighted with 17.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlq/projects/mapillary_sls-main/mapillary_sls/datasets/msls.py:217: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.pIdx = np.asarray(self.pIdx)\n",
      "/home/mlq/projects/mapillary_sls-main/mapillary_sls/datasets/msls.py:218: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.nonNegIdx = np.asarray(self.nonNegIdx)\n"
     ]
    }
   ],
   "source": [
    "posDistThr = 5\n",
    "\n",
    "# negatives are defined outside a radius of 25 m\n",
    "negDistThr = 25\n",
    "\n",
    "# number of negatives per triplet\n",
    "nNeg = 5\n",
    "\n",
    "# number of cached queries\n",
    "cached_queries = 6\n",
    "\n",
    "# number of cached negatives\n",
    "cached_negatives = 100\n",
    "\n",
    "# whether to use positive sampling\n",
    "positive_sampling = True\n",
    "\n",
    "# choose the cities to load\n",
    "cities = SAMPLE_CITIES\n",
    "\n",
    "# choose task to test on [im2im, seq2im, im2seq, seq2seq]\n",
    "task = 'im2im'\n",
    "\n",
    "# choose sequence length\n",
    "seq_length = 1\n",
    "\n",
    "train_dataset = MSLS(root_dir, cities = cities, transform = transform, mode = 'train', task = task, seq_length = seq_length,\n",
    "                    negDistThr = negDistThr, posDistThr = posDistThr, nNeg = nNeg, positive_sampling = positive_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5117d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides dataset into smaller cache sets\n",
    "train_dataset.new_epoch()\n",
    "\n",
    "# creates triplets on the smaller cache set\n",
    "train_dataset.update_subcache()\n",
    "\n",
    "# create data loader\n",
    "opt = {'batch_size': 4, 'shuffle': True}\n",
    "trainDataloader = DataLoader(train_dataset, **opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b370f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(new=False):\n",
    "    \"\"\"get the resnet50\"\"\"\n",
    "    pretrained_net = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    \n",
    "    # if the pretrained_net is not good, use the net\n",
    "    if new == False:\n",
    "        net = pretrained_net\n",
    "    else:\n",
    "        net_list = list(pretrained_net.children())\n",
    "        net_list[-2] = GeMPooling(net_list[-1].in_features)\n",
    "        net = nn.Sequential(*net_list)\n",
    "    return net\n",
    "\n",
    "net = get_net(new=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3dc11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, num_epochs, loss, lr, optimizer, device, task):\n",
    "    \"\"\"train funtion\"\"\"\n",
    "    net.to(device)\n",
    "    loss = loss\n",
    "    optimizer=optimizer\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for j, (sequences, labels) in enumerate(train_iter):\n",
    "            if i == 0:\n",
    "                N = labels.shape[1]\n",
    "                q_seq_length, db_seq_length = split_seq(sequences, N, task)\n",
    "            # sequences.shape=(batch_size, len(q)+len(p)+len(neg), 3, 480, 640)\n",
    "            X = sequences.reshape(-1, 3, 480, 640).to(device)\n",
    "            y_hat = net(X)\n",
    "            y_hat = y_hat.reshape(sequences.shape[0], sequences.shape[1], -1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            anchor = y_hat[:, : q_seq_length, :].mean(1)\n",
    "            positive = y_hat[:, q_seq_length: q_seq_length + db_seq_length, :].mean(1)\n",
    "            negtive = y_hat[:, q_seq_length + db_seq_length:, :].mean(1)\n",
    "            \n",
    "            l = loss(anchor, positive, negtive)\n",
    "            \n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            metric.add(l * sequences.shape[0], labels.numel())\n",
    "            \n",
    "            train_loss = metric[0] / metric[1]\n",
    "            \n",
    "            if j % 10 == 0:\n",
    "                print(f'epoch:{i + 1}, loss:{train_loss:.3f}')\n",
    "                print(list(net.children())[-2].p)\n",
    "        print(f'epoch{i + 1} is end *****************************************')\n",
    "        print(f'now loss:{train_loss:.3f}')\n",
    "        print(f'epoch{i + 2} is start *****************************************')\n",
    "    \n",
    "    print(f'the train is end *****************************************')\n",
    "    print(f'in the end, loss:{train_loss:.3f}')\n",
    "    torch.save(net.state_dict(), f\"train_model_epoch{num_epochs}_lr{lr}sucessfully.params\")\n",
    "    \n",
    "                \n",
    "                    \n",
    "            \n",
    "def split_seq(sequences, N, task):\n",
    "    \"\"\"split the sequences before training according to the task\"\"\"\n",
    "    if task == \"im2im\":\n",
    "        q_seq_length, db_seq_length = 1, 1\n",
    "    elif task == \"seq2seq\":\n",
    "        seq_length = sequences.shape[1] // (N)\n",
    "        q_seq_length, db_seq_length = seq_length, seq_length\n",
    "    elif task == \"im2seq\":\n",
    "        seq_length = (sequences.shape[1] - 1) // (N - 1)\n",
    "        q_seq_length, db_seq_length = 1, seq_length\n",
    "    elif task == \"seq2im\":\n",
    "        seq_length = sequences.shape[1] - (N - 1)\n",
    "        q_seq_length, db_seq_length = seq_length, 1\n",
    "    \n",
    "    return q_seq_length, db_seq_length\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2cd94ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss:1.523\n",
      "Parameter containing:\n",
      "tensor([2.9999, 2.9999, 2.9999,  ..., 2.9999, 3.0001, 2.9999], device='cuda:4',\n",
      "       requires_grad=True)\n",
      "epoch:1, loss:1.169\n",
      "Parameter containing:\n",
      "tensor([3.0002, 2.9994, 2.9992,  ..., 2.9995, 2.9995, 2.9996], device='cuda:4',\n",
      "       requires_grad=True)\n",
      "epoch:1, loss:0.945\n",
      "Parameter containing:\n",
      "tensor([3.0002, 2.9995, 2.9988,  ..., 2.9994, 2.9990, 2.9997], device='cuda:4',\n",
      "       requires_grad=True)\n",
      "epoch:1, loss:0.857\n",
      "Parameter containing:\n",
      "tensor([3.0001, 2.9997, 2.9986,  ..., 2.9997, 2.9985, 2.9995], device='cuda:4',\n",
      "       requires_grad=True)\n",
      "epoch:1, loss:0.749\n",
      "Parameter containing:\n",
      "tensor([2.9998, 2.9996, 2.9984,  ..., 2.9999, 2.9983, 2.9994], device='cuda:4',\n",
      "       requires_grad=True)\n",
      "epoch:1, loss:0.692\n",
      "Parameter containing:\n",
      "tensor([2.9998, 2.9993, 2.9981,  ..., 3.0000, 2.9980, 2.9996], device='cuda:4',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTripletMarginLoss(margin\u001b[39m=\u001b[39mtrain_dataset\u001b[39m.\u001b[39mmargin, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m----> 4\u001b[0m train(net, trainDataloader, num_epochs, loss, lr, optimizer, device, task)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, num_epochs, loss, lr, optimizer, device, task)\u001b[0m\n\u001b[1;32m      8\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m metric \u001b[39m=\u001b[39m d2l\u001b[39m.\u001b[39mAccumulator(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m j, (sequences, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_iter):\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     12\u001b[0m         N \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/mapillary_sls-main/mapillary_sls/datasets/msls.py:467\u001b[0m, in \u001b[0;36mMSLS.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    465\u001b[0m output \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqImages[qidx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)])]\n\u001b[1;32m    466\u001b[0m output\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbImages[pidx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]))\n\u001b[0;32m--> 467\u001b[0m output\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbImages[idx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m nidx])\n\u001b[1;32m    469\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output), torch\u001b[39m.\u001b[39mtensor(target)\n",
      "File \u001b[0;32m~/projects/mapillary_sls-main/mapillary_sls/datasets/msls.py:467\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    465\u001b[0m output \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqImages[qidx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)])]\n\u001b[1;32m    466\u001b[0m output\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbImages[pidx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]))\n\u001b[0;32m--> 467\u001b[0m output\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbImages[idx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m nidx])\n\u001b[1;32m    469\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output), torch\u001b[39m.\u001b[39mtensor(target)\n",
      "File \u001b[0;32m~/projects/mapillary_sls-main/mapillary_sls/datasets/msls.py:467\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    465\u001b[0m output \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqImages[qidx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)])]\n\u001b[1;32m    466\u001b[0m output\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(Image\u001b[39m.\u001b[39mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbImages[pidx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]))\n\u001b[0;32m--> 467\u001b[0m output\u001b[39m.\u001b[39mextend([torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(Image\u001b[39m.\u001b[39;49mopen(im)) \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbImages[idx]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m nidx])\n\u001b[1;32m    469\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output), torch\u001b[39m.\u001b[39mtensor(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/transforms/functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], F_pil\u001b[39m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs, lr, device = 2, 0.0001, torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss = nn.TripletMarginLoss(margin=train_dataset.margin, p=2)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(net, trainDataloader, num_epochs, loss, lr, optimizer, device, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb6dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> zurich\n",
      "=====> sf\n"
     ]
    }
   ],
   "source": [
    "# positive are defined within a radius of 25 m 阳性定义在25米的半径范围内\n",
    "posDistThr = 25\n",
    "\n",
    "# choose task to test on [im2im, seq2im, im2seq, seq2seq]\n",
    "task = 'seq2seq'\n",
    "\n",
    "# choose sequence length\n",
    "seq_length = 3\n",
    "\n",
    "# choose subtask to test on [all, s2w, w2s, o2n, n2o, d2n, n2d]\n",
    "subtask = 'all'\n",
    "\n",
    "val_dataset = MSLS(root_dir, cities = SAMPLE_CITIES, transform = transform, mode = 'test',\n",
    "                   task = task, seq_length = seq_length, subtask = subtask, posDistThr = posDistThr)\n",
    "\n",
    "opt = {'batch_size': 3}\n",
    "\n",
    "# get images\n",
    "qLoader = DataLoader(ImagesFromList(val_dataset.qImages[val_dataset.qIdx], transform), **opt)\n",
    "dbLoader = DataLoader(ImagesFromList(val_dataset.dbImages, transform), **opt)\n",
    "\n",
    "# get positive index (we allow some more slack: default 25 m)\n",
    "pIdx = val_dataset.pIdx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6de14401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([0, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 480, 640])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, batch in enumerate(qLoader):\n",
    "    x, y = batch\n",
    "    # print(len(x))\n",
    "    print(y)\n",
    "    break\n",
    "x[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9ef90c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([1, 3, 480, 640]) tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dbLoader):\n",
    "    x, y = batch\n",
    "    print(len(x))\n",
    "    print(x.shape, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04be3909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = []\n",
    "for i, batch in enumerate(qLoader):\n",
    "    y_list.append(batch[1])\n",
    "len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59ff1131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8006"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydb_list = []\n",
    "for i, batch in enumerate(dbLoader):\n",
    "    ydb_list.append(batch[1])\n",
    "len(ydb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60130fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((361,), (0,), (361,), (7948,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.qIdx.shape, val_dataset.pIdx.shape, val_dataset.qImages.shape, val_dataset.dbImages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d70a7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/datasets/msls/train_val/zurich/database/images/VBhOO_DV9AMtrCBdEg39IA.jpg,/datasets/msls/train_val/zurich/database/images/_qLwDOh1rhtPc7tVsII-wA.jpg,/datasets/msls/train_val/zurich/database/images/-sSqPMpmsbwv9iAjgKb5sQ.jpg',\n",
       "       '/datasets/msls/train_val/zurich/database/images/_qLwDOh1rhtPc7tVsII-wA.jpg,/datasets/msls/train_val/zurich/database/images/-sSqPMpmsbwv9iAjgKb5sQ.jpg,/datasets/msls/train_val/zurich/database/images/5TUQ193fbsXUHn2RmJyIUQ.jpg',\n",
       "       '/datasets/msls/train_val/zurich/database/images/-sSqPMpmsbwv9iAjgKb5sQ.jpg,/datasets/msls/train_val/zurich/database/images/5TUQ193fbsXUHn2RmJyIUQ.jpg,/datasets/msls/train_val/zurich/database/images/P_7zNYGjYObsCIpaM7e3Kg.jpg'],\n",
       "      dtype='<U224')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.dbImages[batch[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "466315e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5125,  1.5125,  1.5297,  ..., -2.0837, -1.9980, -2.1179],\n",
       "          [ 1.5125,  1.5125,  1.5297,  ..., -1.9980, -1.9124, -2.0494],\n",
       "          [ 1.4954,  1.4954,  1.5125,  ..., -2.1008, -2.1008, -2.0837],\n",
       "          ...,\n",
       "          [-1.3130, -1.2445, -1.1418,  ..., -1.2959, -1.2788, -1.2103],\n",
       "          [-1.4329, -1.3815, -1.2788,  ..., -1.3130, -1.3302, -1.3302],\n",
       "          [-1.3987, -1.4158, -1.3987,  ..., -1.5870, -1.6213, -1.6898]],\n",
       "\n",
       "         [[ 1.6583,  1.6583,  1.6758,  ..., -2.0007, -1.9132, -2.0357],\n",
       "          [ 1.6583,  1.6583,  1.6758,  ..., -1.9132, -1.8256, -1.9657],\n",
       "          [ 1.6408,  1.6408,  1.6583,  ..., -2.0182, -2.0182, -2.0007],\n",
       "          ...,\n",
       "          [-1.2479, -1.1779, -1.0728,  ..., -1.1253, -1.1078, -1.0378],\n",
       "          [-1.3704, -1.3179, -1.2129,  ..., -1.1604, -1.1779, -1.1779],\n",
       "          [-1.3354, -1.3529, -1.3354,  ..., -1.4405, -1.4755, -1.5455]],\n",
       "\n",
       "         [[ 2.1520,  2.1520,  2.1694,  ..., -1.7696, -1.6824, -1.8044],\n",
       "          [ 2.1520,  2.1520,  2.1694,  ..., -1.6824, -1.5953, -1.7347],\n",
       "          [ 2.1346,  2.1346,  2.1520,  ..., -1.7870, -1.7870, -1.7696],\n",
       "          ...,\n",
       "          [-0.8284, -0.7587, -0.6541,  ..., -1.4733, -1.4559, -1.3861],\n",
       "          [-0.9504, -0.8981, -0.7936,  ..., -1.4733, -1.4907, -1.4907],\n",
       "          [-0.9156, -0.9330, -0.9156,  ..., -1.7522, -1.7870, -1.8044]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "957fc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.io.read_image(\"D:\\\\MSLS_train_val\\\\train_val\\\\trondheim\\\\database\\\\images\\\\9Iu7ckykQxh2KCQlhmraUg.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c26ba7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5125,  1.5125,  1.5297,  ..., -2.0837, -1.9980, -2.1179],\n",
       "         [ 1.5125,  1.5125,  1.5297,  ..., -1.9980, -1.9124, -2.0494],\n",
       "         [ 1.4954,  1.4954,  1.5125,  ..., -2.1008, -2.1008, -2.0837],\n",
       "         ...,\n",
       "         [-1.3130, -1.2445, -1.1418,  ..., -1.2959, -1.2788, -1.2103],\n",
       "         [-1.4329, -1.3815, -1.2788,  ..., -1.3130, -1.3302, -1.3302],\n",
       "         [-1.3987, -1.4158, -1.3987,  ..., -1.5870, -1.6213, -1.6898]],\n",
       "\n",
       "        [[ 1.6583,  1.6583,  1.6758,  ..., -2.0007, -1.9132, -2.0357],\n",
       "         [ 1.6583,  1.6583,  1.6758,  ..., -1.9132, -1.8256, -1.9657],\n",
       "         [ 1.6408,  1.6408,  1.6583,  ..., -2.0182, -2.0182, -2.0007],\n",
       "         ...,\n",
       "         [-1.2479, -1.1779, -1.0728,  ..., -1.1253, -1.1078, -1.0378],\n",
       "         [-1.3704, -1.3179, -1.2129,  ..., -1.1604, -1.1779, -1.1779],\n",
       "         [-1.3354, -1.3529, -1.3354,  ..., -1.4405, -1.4755, -1.5455]],\n",
       "\n",
       "        [[ 2.1520,  2.1520,  2.1694,  ..., -1.7696, -1.6824, -1.8044],\n",
       "         [ 2.1520,  2.1520,  2.1694,  ..., -1.6824, -1.5953, -1.7347],\n",
       "         [ 2.1346,  2.1346,  2.1520,  ..., -1.7870, -1.7870, -1.7696],\n",
       "         ...,\n",
       "         [-0.8284, -0.7587, -0.6541,  ..., -1.4733, -1.4559, -1.3861],\n",
       "         [-0.9504, -0.8981, -0.7936,  ..., -1.4733, -1.4907, -1.4907],\n",
       "         [-0.9156, -0.9330, -0.9156,  ..., -1.7522, -1.7870, -1.8044]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"D:\\\\MSLS_train_val\\\\train_val\\\\trondheim\\\\database\\\\images\\\\9Iu7ckykQxh2KCQlhmraUg.jpg\")\n",
    "img = transform(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d089d5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed51e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_result.shape, q_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c70ba317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_feature(net, Loader, device):\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    result = []\n",
    "    idx = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in Loader:\n",
    "            x = x.to(device)\n",
    "            y_hat = net(x)\n",
    "            result.append(y_hat)\n",
    "            idx.append(y)  \n",
    "        result = torch.cat(result, dim=0)\n",
    "        idx = torch.cat(idx, dim=0)\n",
    "    return result, idx\n",
    "#q_result, q_idx = predict_feature(net, qLoader, val_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b9b993cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([362, 1000]), torch.Size([362]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_result.shape, q_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba274a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_result, db_idx = predict_feature(net_trained, dbLoader, device)\n",
    "def query_to_dbIdx(q_result, db_result):\n",
    "    # save the indices of the first 5 most similar images to query in db\n",
    "    img_indices_list = []\n",
    "    for feature in q_result:\n",
    "        diff = torch.abs(feature - db_result).sum(dim=1).reshape(-1)\n",
    "        # get the index of the first 5 minimum in diff\n",
    "        idx = torch.argsort(diff)[:5].reshape(1, -1)\n",
    "        \n",
    "        img_indices_list.append(idx)\n",
    "    img_idices = torch.cat(img_indices_list, dim=0)\n",
    "    \n",
    "    return img_idices\n",
    "\n",
    "\n",
    "def find_keys(indices, val_dataset, mode=\"query\"):\n",
    "    \n",
    "    if mode == \"query\":\n",
    "        address_all = val_dataset.qImages[indices]\n",
    "        # save the keys of all queries, one key for one query\n",
    "        keys = []\n",
    "        for address_query in address_all:\n",
    "            # address_query示例：D:\\\\MSLS_train_val\\\\train_val\\\\zurich\\\\database\\\\images\\\\_qLwDOh1rhtPc7tVsII-wA.jpg\n",
    "            key = address_query.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "            keys.append(key)\n",
    "        \n",
    "        assert len(keys) == indices.shape[0]\n",
    "        return keys\n",
    "        \n",
    "    elif mode == \"database\":\n",
    "        # get all the addresses of the first 5 most similar imgs for all the queries\n",
    "        address_all = val_dataset.dbImages[indices]\n",
    "        \n",
    "        # save the keys of all the queries\n",
    "        keys_all = []\n",
    "        for address_query in address_all:\n",
    "            # save the 5 keys of one query\n",
    "            keys = []\n",
    "            for address in address_query:\n",
    "                # address示例：D:\\\\MSLS_train_val\\\\train_val\\\\zurich\\\\database\\\\images\\\\_qLwDOh1rhtPc7tVsII-wA.jpg\n",
    "                key = address.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "                keys.append(key)\n",
    "            keys_all.append(keys)\n",
    "            \n",
    "    return keys_all\n",
    "    \n",
    "q1_idx = torch.arange(0,109, 1)\n",
    "q1 = torch.randn(size=(109, 5))\n",
    "db1 = torch.randn(size=(200, 5))\n",
    "db_indices = query_to_dbIdx(q1, db1)\n",
    "# find_keys according to the index(index, dataset, mode=\"query\"/\"database\")\n",
    "db_keys = find_keys(db_indices, val_dataset, mode=\"database\")\n",
    "q_keys = find_keys(q1_idx, val_dataset, mode=\"query\")\n",
    "db_keys, q_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aae58740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9sE-UeCra7KgJx6rNkq9xQ',\n",
       " 'D:\\\\MSLS_train_val\\\\train_val\\\\zurich\\\\database\\\\images\\\\_qLwDOh1rhtPc7tVsII-wA.jpg')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in img_address:\n",
    "    a = i\n",
    "num=2\n",
    "a[num].split(\"\\\\\")[-1].split(\".\")[0], a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4e7b2066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AoD5-ZB5YrgyClbR5qmG4g\n",
      "gEhnDp9LMQq4SEDWgyVvQw\n",
      "mguDPMkLcTvYctYS2Zvo1w\n"
     ]
    }
   ],
   "source": [
    "for i in q_keys:\n",
    "    a = i.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "79d420fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(q_keys, db_keys):\n",
    "    # create the csv saved keys\n",
    "    os.makedirs(os.path.join('.', 'files'), exist_ok=True)\n",
    "    data_file = os.path.join('.', 'files', 'my_prediction_im2im.csv')\n",
    "    \n",
    "    with open(data_file, 'w') as f:\n",
    "        # one query key match to five database keys\n",
    "        for line, q_key in zip(db_keys, q_keys):\n",
    "            f.write(str(q_key) + ' ')\n",
    "            for db_key in line:\n",
    "                f.write(str(db_key) + ' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "save_to_csv(q_keys, db_keys)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b3952e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\2121940830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdb_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\427069581.py\u001b[0m in \u001b[0;36mpredict_feature\u001b[1;34m(net, Loader, val_dataset, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mLoader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0midx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         )\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2450\u001b[0m     return torch.batch_norm(\n\u001b[1;32m-> 2451\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2452\u001b[0m     )\n\u001b[0;32m   2453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1878adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45e01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf0327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffac445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad679e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
