{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import evaluate\n",
    "import torchvision\n",
    "import clip\n",
    "from modules.GeMPooling import GeMPooling\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from mapillary_sls.datasets.msls import MSLS\n",
    "from mapillary_sls.datasets.msls_clip import MSLSCLIP\n",
    "from mapillary_sls.datasets.generic_dataset import ImagesFromList, ImagesText\n",
    "from mapillary_sls.utils.utils import configure_transform, clip_transform\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = \"sf,cph\"\n",
    "root_dir = Path('/root/autodl-tmp/msls').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> sf\n",
      "=====> cph\n"
     ]
    }
   ],
   "source": [
    "image_dim = (224, 224)\n",
    "transform = clip_transform(image_dim)\n",
    "# positive are defined within a radius of 25 m 阳性定义在25米的半径范围内\n",
    "posDistThr = 25\n",
    "\n",
    "# choose task to test on [im2im, seq2im, im2seq, seq2seq]\n",
    "task = 'im2im'\n",
    "\n",
    "# choose sequence length\n",
    "seq_length = 1\n",
    "\n",
    "# choose subtask to test on [all, s2w, w2s, o2n, n2o, d2n, n2d]\n",
    "subtask = 'all'\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "val_dataset = MSLSCLIP(root_dir, cities = cities, transform = transform, mode = 'test',\n",
    "                        task = task, seq_length = seq_length, subtask = subtask, posDistThr = posDistThr)\n",
    "        \n",
    "opt = {'batch_size': batch_size}\n",
    "# get images\n",
    "qLoader = DataLoader(ImagesText(val_dataset.qImages[val_dataset.qIdx], val_dataset.qText[val_dataset.qIdx], transform), **opt)\n",
    "dbLoader = DataLoader(ImagesText(val_dataset.dbImages, val_dataset.dbText, transform), **opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clip_feature(net, Loader, device, im_or_seq='im'):\n",
    "    \"\"\"compute the features with net trained and get indices\"\"\"\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    result = []\n",
    "    idx = []\n",
    "    with torch.no_grad():\n",
    "        if im_or_seq == 'im':\n",
    "\n",
    "            for img_txt, y in Loader:\n",
    "                x, text = img_txt\n",
    "                print(x.shape)\n",
    "                print(\"***\")\n",
    "                print(text.shape)\n",
    "                x, text = x.to(device), text.reshape(-1, text.shape[-1]).to(device)\n",
    "                \n",
    "                y_hat = net(x, text)\n",
    "                \n",
    "                result.append(y_hat)\n",
    "                idx.append(y)\n",
    "                break\n",
    "        elif im_or_seq == 'seq':\n",
    "            # type(x_list)=list, and len(x_list=seq_length)\n",
    "            for x_list, y in Loader:\n",
    "                y_hat_list = torch.zeros((x_list[0].shape[0], net.back[1].out_features)).to(device)\n",
    "                seq_length = len(x_list)\n",
    "                for x in x_list:\n",
    "                    # now the shape of x is(batch_size, 3, 224, 224)\n",
    "                    x = x.to(device)\n",
    "                    y_hat = net(x)\n",
    "                    # compute the mean of all images in the seq\n",
    "                    y_hat_list += y_hat\n",
    "                y_hat = y_hat_list / seq_length\n",
    "                result.append(y_hat)\n",
    "                idx.append(y)\n",
    "    result = torch.cat(result, dim=0)\n",
    "    idx = torch.cat(idx, dim=0).reshape(-1, 1)\n",
    "    return result, idx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, _= clip.load(\"ViT-B/16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "***\n",
      "torch.Size([4, 1, 77])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "q_feature, q_idx = predict_clip_feature(net, qLoader, device, task.split('2')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18916\n",
      "18871\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(len(val_dataset.dbImages))\n",
    "print(len(val_dataset.dbText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
