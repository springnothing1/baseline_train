{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import llama\n",
    "import clipvpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 23.15 GiB already allocated; 20.06 MiB free; 23.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/root/baseline_train/clipvpr.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/baseline_train/clipvpr.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/baseline_train/clipvpr.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model, process \u001b[39m=\u001b[39m clipvpr\u001b[39m.\u001b[39;49mload(clip_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mViT-B/16\u001b[39;49m\u001b[39m\"\u001b[39;49m, device \u001b[39m=\u001b[39;49m device, llama_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBIAS-7B\u001b[39;49m\u001b[39m\"\u001b[39;49m, llama_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./path/to/LLaMA/\u001b[39;49m\u001b[39m'\u001b[39;49m, llama_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m7B\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bconnect.westc.gpuhub.com/root/baseline_train/clipvpr.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m          llama_download_root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mckpts\u001b[39;49m\u001b[39m'\u001b[39;49m, max_seq_len\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, phase\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfinetune\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/baseline_train/clipvpr/clipvpr.py:107\u001b[0m, in \u001b[0;36mload\u001b[0;34m(clip_name, device, jit, clip_download_root, llama_name, llama_dir, llama_type, llama_download_root, max_seq_len, phase, prompt)\u001b[0m\n\u001b[1;32m    104\u001b[0m ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path, map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m model_cfg \u001b[39m=\u001b[39m ckpt\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m, {})\n\u001b[0;32m--> 107\u001b[0m model \u001b[39m=\u001b[39m build_model(clip_model\u001b[39m.\u001b[39mstate_dict() \u001b[39mif\u001b[39;00m clip_state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m clip_state_dict,\n\u001b[1;32m    108\u001b[0m                     prompt, llama_ckpt_dir,llama_tokenzier_path,model_cfg, max_seq_len, phase)\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    110\u001b[0m     model\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/baseline_train/clipvpr/model.py:499\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(state_dict, prompts, llama_ckpt_dir, llama_tokenzier_path, model_cfg, max_seq_len, phase)\u001b[0m\n\u001b[1;32m    494\u001b[0m transformer_heads \u001b[39m=\u001b[39m transformer_width \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m    495\u001b[0m transformer_layers \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(k\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m state_dict \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtransformer.resblocks\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m--> 499\u001b[0m llama_model \u001b[39m=\u001b[39m llama\u001b[39m.\u001b[39;49mLLaMA_adapter(\n\u001b[1;32m    500\u001b[0m     llama_ckpt_dir, llama_tokenzier_path,\n\u001b[1;32m    501\u001b[0m     max_seq_len\u001b[39m=\u001b[39;49mmax_seq_len, max_batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    502\u001b[0m     clip_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mViT-L/14\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    503\u001b[0m     v_embed_dim\u001b[39m=\u001b[39;49m\u001b[39m768\u001b[39;49m, v_depth\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m    504\u001b[0m     v_num_heads\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, v_mlp_ratio\u001b[39m=\u001b[39;49m\u001b[39m4.0\u001b[39;49m,\n\u001b[1;32m    505\u001b[0m     query_len\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, query_layer\u001b[39m=\u001b[39;49m\u001b[39m31\u001b[39;49m,\n\u001b[1;32m    506\u001b[0m     w_bias\u001b[39m=\u001b[39;49mmodel_cfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mw_bias\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m), \n\u001b[1;32m    507\u001b[0m     w_lora\u001b[39m=\u001b[39;49mmodel_cfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mw_lora\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m), \n\u001b[1;32m    508\u001b[0m     lora_rank\u001b[39m=\u001b[39;49mmodel_cfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mlora_rank\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m16\u001b[39;49m),\n\u001b[1;32m    509\u001b[0m     w_new_gate\u001b[39m=\u001b[39;49mmodel_cfg\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mw_lora\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m), \u001b[39m# for compatibility\u001b[39;49;00m\n\u001b[1;32m    510\u001b[0m     phase\u001b[39m=\u001b[39;49mphase\n\u001b[1;32m    511\u001b[0m     )\n\u001b[1;32m    513\u001b[0m model \u001b[39m=\u001b[39m CLIP(\n\u001b[1;32m    514\u001b[0m     embed_dim,\n\u001b[1;32m    515\u001b[0m     image_resolution, vision_layers, vision_width, vision_patch_size,\n\u001b[1;32m    516\u001b[0m     context_length, vocab_size, transformer_width, transformer_heads, transformer_layers,\n\u001b[1;32m    517\u001b[0m     llama_model, prompts\n\u001b[1;32m    518\u001b[0m )\n\u001b[1;32m    520\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39minput_resolution\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontext_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/baseline_train/llama/llama_adapter.py:38\u001b[0m, in \u001b[0;36mLLaMA_adapter.__init__\u001b[0;34m(self, llama_ckpt_dir, llama_tokenizer, max_seq_len, max_batch_size, clip_model, v_embed_dim, v_depth, v_num_heads, v_mlp_ratio, query_len, query_layer, w_bias, w_lora, lora_rank, w_new_gate, phase)\u001b[0m\n\u001b[1;32m     33\u001b[0m model_args: ModelArgs \u001b[39m=\u001b[39m ModelArgs(\n\u001b[1;32m     34\u001b[0m     max_seq_len\u001b[39m=\u001b[39mmax_seq_len, max_batch_size\u001b[39m=\u001b[39mmax_batch_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m     35\u001b[0m ) \u001b[39m# max_batch_size only affects inferenc\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# 1. clip and clip projector\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_transform \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mload(clip_model)\n\u001b[1;32m     40\u001b[0m clip_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip\u001b[39m.\u001b[39mvisual\u001b[39m.\u001b[39mproj\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(clip_dim, v_embed_dim)\n",
      "File \u001b[0;32m~/baseline_train/clip/clip.py:139\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m    136\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(opened_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m jit:\n\u001b[0;32m--> 139\u001b[0m     model \u001b[39m=\u001b[39m build_model(state_dict \u001b[39mor\u001b[39;49;00m model\u001b[39m.\u001b[39;49mstate_dict())\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    141\u001b[0m         model\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/baseline_train/clip/model.py:427\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(state_dict)\u001b[0m\n\u001b[1;32m    424\u001b[0m transformer_heads \u001b[39m=\u001b[39m transformer_width \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m    425\u001b[0m transformer_layers \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(k\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m state_dict \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtransformer.resblocks\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m--> 427\u001b[0m model \u001b[39m=\u001b[39m CLIP(\n\u001b[1;32m    428\u001b[0m     embed_dim,\n\u001b[1;32m    429\u001b[0m     image_resolution, vision_layers, vision_width, vision_patch_size,\n\u001b[1;32m    430\u001b[0m     context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    433\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39minput_resolution\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontext_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/baseline_train/clip/model.py:272\u001b[0m, in \u001b[0;36mCLIP.__init__\u001b[0;34m(self, embed_dim, image_resolution, vision_layers, vision_width, vision_patch_size, context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     vision_heads \u001b[39m=\u001b[39m vision_width \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual \u001b[39m=\u001b[39m VisionTransformer(\n\u001b[1;32m    273\u001b[0m         input_resolution\u001b[39m=\u001b[39;49mimage_resolution,\n\u001b[1;32m    274\u001b[0m         patch_size\u001b[39m=\u001b[39;49mvision_patch_size,\n\u001b[1;32m    275\u001b[0m         width\u001b[39m=\u001b[39;49mvision_width,\n\u001b[1;32m    276\u001b[0m         layers\u001b[39m=\u001b[39;49mvision_layers,\n\u001b[1;32m    277\u001b[0m         heads\u001b[39m=\u001b[39;49mvision_heads,\n\u001b[1;32m    278\u001b[0m         output_dim\u001b[39m=\u001b[39;49membed_dim\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m Transformer(\n\u001b[1;32m    282\u001b[0m     width\u001b[39m=\u001b[39mtransformer_width,\n\u001b[1;32m    283\u001b[0m     layers\u001b[39m=\u001b[39mtransformer_layers,\n\u001b[1;32m    284\u001b[0m     heads\u001b[39m=\u001b[39mtransformer_heads,\n\u001b[1;32m    285\u001b[0m     attn_mask\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_attention_mask()\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m vocab_size   \u001b[39m# should be 77\u001b[39;00m\n",
      "File \u001b[0;32m~/baseline_train/clip/model.py:217\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[0;34m(self, input_resolution, patch_size, width, layers, heads, output_dim)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(scale \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrandn((input_resolution \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m patch_size) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, width))\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre \u001b[39m=\u001b[39m LayerNorm(width)\n\u001b[0;32m--> 217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m Transformer(width, layers, heads)\n\u001b[1;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post \u001b[39m=\u001b[39m LayerNorm(width)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(scale \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrandn(width, output_dim))\n",
      "File \u001b[0;32m~/baseline_train/clip/model.py:200\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, width, layers, heads, attn_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m width\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m layers\n\u001b[0;32m--> 200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[ResidualAttentionBlock(width, heads, attn_mask) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(layers)])\n",
      "File \u001b[0;32m~/baseline_train/clip/model.py:200\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m width\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m layers\n\u001b[0;32m--> 200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m[ResidualAttentionBlock(width, heads, attn_mask) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(layers)])\n",
      "File \u001b[0;32m~/baseline_train/clip/model.py:178\u001b[0m, in \u001b[0;36mResidualAttentionBlock.__init__\u001b[0;34m(self, d_model, n_head, attn_mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMultiheadAttention(d_model, n_head)\n\u001b[1;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1 \u001b[39m=\u001b[39m LayerNorm(d_model)\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(OrderedDict([\n\u001b[0;32m--> 178\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mc_fc\u001b[39m\u001b[39m\"\u001b[39m, nn\u001b[39m.\u001b[39;49mLinear(d_model, d_model \u001b[39m*\u001b[39;49m \u001b[39m4\u001b[39;49m)),\n\u001b[1;32m    179\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mgelu\u001b[39m\u001b[39m\"\u001b[39m, QuickGELU()),\n\u001b[1;32m    180\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mc_proj\u001b[39m\u001b[39m\"\u001b[39m, nn\u001b[39m.\u001b[39mLinear(d_model \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, d_model))\n\u001b[1;32m    181\u001b[0m ]))\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2 \u001b[39m=\u001b[39m LayerNorm(d_model)\n\u001b[1;32m    183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask \u001b[39m=\u001b[39m attn_mask\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 23.15 GiB already allocated; 20.06 MiB free; 23.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, process = clipvpr.load(clip_name=\"ViT-B/16\", device = device, llama_name=\"BIAS-7B\", llama_dir='./path/to/LLaMA/', llama_type=\"7B\", \n",
    "         llama_download_root='ckpts', max_seq_len=512, phase=\"finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda().eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
